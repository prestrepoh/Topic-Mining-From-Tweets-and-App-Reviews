{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"models_training.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"YL-HOa1tC0QH"},"source":["# Environment Preparation"]},{"cell_type":"markdown","metadata":{"id":"q2qbUBQfI3uS"},"source":["## Mount Drive and Install Dependencies"]},{"cell_type":"code","metadata":{"id":"dmc46cQjLUPh"},"source":["#Mount Drive and install dependencies\n","def install_dependecies():\n","  !pip install transformers==4.6.1\n","  !pip install pytorch-lightning==1.3.5\n","  !pip install optuna==2.8.0\n","\n","from sys import path\n","import os\n","import sys\n","\n","if 'google.colab' in str(get_ipython()):\n","  from google.colab import drive\n","\n","  root_PATH = '/content/drive/My Drive/topic-mining-paper/repository'\n","  drive_mount_location = '/content/drive'\n","  module_path = root_PATH + '/src'\n","  \n","  drive.mount(drive_mount_location, force_remount=True)\n","  path.append(root_PATH)\n","\n","  install_dependecies()\n","else:\n","  root_PATH = os.path.abspath(\"../..\")\n","  module_path = os.path.abspath(os.path.join('../../src'))\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","if module_path not in sys.path:\n","    sys.path.append(module_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wP6mKa51MyP5"},"source":["import pandas as pd\n","from torch import cuda\n","from transformers import BertTokenizer, BertModel, BertConfig\n","import torch\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, WeightedRandomSampler\n","import torch.nn as nn\n","import transformers\n","import numpy as np\n","from sklearn import metrics\n","import pytorch_lightning as pl\n","from pytorch_lightning import Trainer\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","from sklearn.metrics import accuracy_score\n","from pytorch_lightning.loggers import TensorBoardLogger\n","import gc \n","import optuna"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S85PxFMhg_eI"},"source":["## Helper Classes"]},{"cell_type":"code","metadata":{"id":"_R88Yl8AMzpQ"},"source":["import torch\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","\n","class CustomDataset(Dataset):\n","\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.comment_text = dataframe.text\n","        self.targets = dataframe.target\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.comment_text)\n","\n","    def __getitem__(self, index):\n","        \n","        comment_text = str(self.comment_text[index])\n","\n","        inputs = self.tokenizer.encode_plus(\n","            comment_text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","          \tpadding='max_length',\n","            return_token_type_ids=True,\n","            truncation = True\n","        )\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs[\"token_type_ids\"]\n","\n","\n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'targets': torch.tensor(self.targets[index], dtype=torch.long)\n","        }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3BxlDhqXM4PC"},"source":["import pytorch_lightning as pl\n","import torch\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","import torch.nn as nn\n","import numpy as np\n","import abc\n"," \n","from re import T\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_fscore_support\n","import pandas as pd\n"," \n","class CustomModel(pl.LightningModule):\n"," \n","    def __init__(self, hyperparams, training_dataset, validation_dataset, labels, model_to_use, tokenizer, training_sampler):\n","        super().__init__()\n"," \n","        self.hyperparams = hyperparams\n","        self.training_dataset = training_dataset\n","        self.validation_dataset = validation_dataset\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.define_model(model_to_use, tokenizer)\n","        self.training_sampler = training_sampler\n"," \n","        self.best_f1 = 0\n","        self.epoch_best_f1 = 0\n"," \n","    @abc.abstractmethod\n","    def define_model(self, model_to_use, tokenizer):\n","        pass\n"," \n","    @abc.abstractmethod\n","    def forward(self, ids, mask, token_type_ids):\n","        pass\n"," \n","    def loss_fn(self, outputs, targets):\n","      return torch.nn.CrossEntropyLoss()(outputs, targets)\n"," \n","    def general_step(self, batch, batch_idx, mode):\n","      ids = batch['ids']\n","      mask = batch['mask']\n","      token_type_ids = batch['token_type_ids']\n","      targets = batch['targets']\n"," \n","      outputs = self.forward(ids, mask, token_type_ids)\n"," \n","      return {'outputs': outputs, 'targets': targets}\n"," \n","    #******Training******\n","    #This method runs on each GPU\n","    def training_step(self, batch, batch_idx):\n","      return self.general_step(batch, batch_idx, \"train\")\n","    \n","    #This method aggregates the results of training_step in the different GPUs\n","    def training_step_end(self, aggregated_outputs):\n","      loss = self.loss_fn(aggregated_outputs[\"outputs\"], aggregated_outputs[\"targets\"])\n","      self.log('training_loss',loss)\n","      return {'loss':loss}\n","    \n","    #This method runs at the end of each epoch\n","    def training_epoch_end(self, results_of_each_batch):\n","      pass\n"," \n","    #******Validation******\n","    #This method runs in each GPU\n","    def validation_step(self, batch, batch_idx):\n","      return self.general_step(batch, batch_idx, \"val\")\n"," \n","    #This method aggregates the results of validation_step in the different GPUs\n","    def validation_step_end (self, aggregated_outputs):\n","      outputs = torch.tensor(aggregated_outputs['outputs'].cpu().detach().numpy().tolist())\n","      prediction_values, prediction_indexes = torch.max(outputs, dim=1)\n","      targets = aggregated_outputs['targets'].cpu().detach().numpy()\n"," \n","      return {'predictions': prediction_indexes, 'targets': targets}\n"," \n","    #This method runs at the end of each epoch\n","    def validation_epoch_end(self, results_of_each_batch):\n","      targets = np.empty([0])\n","      predictions = np.empty([0])\n","      \n","      for result in results_of_each_batch:\n","        targets = np.concatenate((targets,result['targets']))\n","        predictions = np.concatenate((predictions,result['predictions']))\n","      \n","      accuracy = accuracy_score(targets, predictions)\n","      precision, recall, f1, _ = precision_recall_fscore_support(targets, \n","                                                                 predictions, \n","                                                                 average=\"binary\", \n","                                                                 pos_label=1,\n","                                                                 zero_division=0)\n","      auc_score = 0\n","      try:\n","        auc_score = roc_auc_score(targets, predictions)\n","      except:\n","        pass\n"," \n","      results = { 'accuracy':accuracy,\n","                  'precision':precision,\n","                  'recall':recall,\n","                  'f1':f1,\n","                  'auc':auc_score\n","                }\n"," \n","      self.log('f1', f1)\n"," \n","      if f1 > self.best_f1:\n","        self.best_f1 = f1\n","        self.epoch_best_f1 = self.current_epoch\n"," \n","      print(f'epoch {self.current_epoch}: {results}')\n"," \n","    def configure_optimizers(self):\n","      return torch.optim.Adam(params = self.parameters(), lr=self.hyperparams[\"learning_rate\"])\n"," \n","    #******Dataloaders******\n","    def train_dataloader(self):\n","      if self.training_sampler is None:\n","        shuffle = self.hyperparams[\"shuffle\"]\n","      else:\n","        shuffle = False\n","      \n","      return DataLoader(self.training_dataset, \n","                        batch_size=self.hyperparams[\"train_batch_size\"], \n","                        shuffle=shuffle, \n","                        num_workers=2, \n","                        sampler=self.training_sampler )\n"," \n","    def val_dataloader(self):\n","      return DataLoader(self.validation_dataset, \n","                        batch_size=self.hyperparams[\"validation_batch_size\"], \n","                        shuffle= False, \n","                        num_workers=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fg__4VLqM9cJ"},"source":["from transformers import BertTokenizer, BertModel, BertConfig\n","import transformers\n","import torch\n"," \n","class BERTCustomModel(CustomModel):\n"," \n","    def define_model(self, model_to_use, tokenizer):\n","      self.l1 = transformers.BertModel.from_pretrained(model_to_use)\n","      self.l1.resize_token_embeddings(len(tokenizer)) \n"," \n","      self.l2 = torch.nn.Dropout(0.3)\n","      self.l3 = torch.nn.Linear(768, len(self.labels))\n"," \n","    def forward(self, ids, mask, token_type_ids):\n"," \n","      transformer_output = self.l1(input_ids = ids, token_type_ids = token_type_ids, attention_mask= mask)\n","      hidden_state = transformer_output.last_hidden_state\n","      \n","      output_1 = hidden_state[:, 0] #CLS token\n","      output_2 = self.l2(output_1)\n","      output = self.l3(output_2)\n","      \n","      return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0tZQFM9pkD8-"},"source":["from enum import Enum\n"," \n","logs_PATH = '/tmp'\n"," \n","def category_to_number(category):\n","  result = 0\n"," \n","  if category == 'irr':\n","      result = 0\n","  elif category == 'pbr':\n","      result = 1\n","  elif category == 'inq':\n","      result = 2\n","  return result\n"," \n","def define_target(row, category):\n","  if row.category == category:\n","    return 1\n","  else:\n","    return 0\n"," \n","class Category(Enum):\n","    PROBLEM = 'pbr'\n","    INQUIRY = 'inq'\n","    IRRELEVANT = 'irr'\n"," \n","class Language(Enum):\n","    ENGLISH = 'en'\n","    ITALIAN = 'it'\n"," \n","class Source(Enum):\n","    APP = 'app_review'\n","    TWITTER = 'tweet'\n"," \n","def train_model(train_df, validation_df, model_to_use, model_params, logs_path, undersampling, max_epochs,save_checkpoints, early_stop, gpus_to_use,max_len):\n"," \n","    if undersampling:\n","      #Sampler Configuration\n","      class_count = train_df['target'].value_counts().values\n","      target_list = train_df['target'].tolist()\n"," \n","      class_weights = 1./torch.tensor(class_count, dtype=torch.float)\n","      class_weights_all = class_weights[target_list]\n"," \n","      weighted_sampler = WeightedRandomSampler(\n","          weights=class_weights_all,\n","          num_samples=len(class_weights_all),\n","          replacement=True\n","      )\n","    else:\n","      weighted_sampler = None\n"," \n","    #Tokenizer\n","    tokenizer = BertTokenizer.from_pretrained(model_to_use)\n","    tokenizer.add_tokens(\"@mention\",special_tokens=False)\n"," \n","    #Datasets\n","    training_set = CustomDataset(train_df, tokenizer, max_len)\n","    validation_set = CustomDataset(validation_df, tokenizer, max_len)\n"," \n","    #Training\n","    labels = [0,1]\n","    logger = TensorBoardLogger(logs_path, name='my_BERT_model')\n","    model = BERTCustomModel(model_params, \n","                            training_set, \n","                            validation_set, \n","                            labels, \n","                            model_to_use,\n","                            tokenizer,\n","                            weighted_sampler)\n","    \n","    callbacks = []\n","\n","    if save_checkpoints:\n","      checkpoint_callback = ModelCheckpoint(\n","                                          monitor='f1',\n","                                          dirpath=logs_path,\n","                                          save_top_k=1,\n","                                          mode='max',\n","                                          )\n","      callbacks.append(checkpoint_callback)\n","\n","    if early_stop:\n","      early_stop_callback = pl.callbacks.EarlyStopping('f1', patience=1)\n","      callbacks.append(early_stop_callback)\n","\n","    trainer = pl.Trainer(gpus=gpus_to_use,\n","                                #accelerator='dp',\n","                                #limit_train_batches=2,\n","                                #limit_val_batches=2,\n","                                #limit_test_batches=32,\n","                                #logger=logger,\n","                                checkpoint_callback=save_checkpoints,\n","                                max_epochs=max_epochs,\n","                                #default_root_dir=logs_path,\n","                                #deterministic=True,\n","                                #callbacks=[checkpoint_callback]\n","                                callbacks = callbacks\n","                                )\n","    \n","    trainer.fit(model)\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    #print(f'best f1: {model.best_f1}, best epoch: {model.epoch_best_f1}')\n","    return model.best_f1\n","\n","from sklearn.model_selection import KFold\n","\n","def objective(trial, dataset, validation_batch_size, shuffle, gpus_to_use,max_len, train_batch_size):\n","\n","  max_epochs = 2\n","  save_checkpoints = False\n","  early_stop = False\n","\n","  model_params = {\n","                    'learning_rate': trial.suggest_uniform(\"LEARNING_RATE\", 1e-05, 1e-04), \n","                    'train_batch_size': train_batch_size, \n","                    'validation_batch_size': validation_batch_size,\n","                    'shuffle': shuffle\n","                }\n","\n","  #Perform cross-validation\n","  fold = KFold(n_splits=3, shuffle=True, random_state=0)\n","  scores = []\n","  splits = fold.split(dataset)\n","\n","  for split in splits:\n","    train_df = dataset.iloc[split[0]].reset_index(drop=True)\n","    validation_df = dataset.iloc[split[1]].reset_index(drop=True)\n","\n","    logs_path = \"/tmp/hyperparam-optim/logs\"\n","\n","    best_f1 = train_model(train_df, validation_df, model_to_use, model_params,logs_path,undersampling,max_epochs,save_checkpoints,early_stop,gpus_to_use,max_len)\n","    \n","    scores.append(best_f1)\n","\n","  return np.mean(scores)\n","\n","\n","def train_models_all_categories(language,source,model_to_use,data_df,undersampling, multilingual_model, experiment_name):\n","  for category in Category:\n","    print(f'Model: {model_to_use}, Language: {language.value}, Source: {source.value}, Category: {category.value}({category_to_number(category.value)}), Undersampling: {undersampling}, Multilingual: {multilingual_model}')\n","    \n","    logs_path = logs_PATH + f'/logs/{experiment_name}/{model_to_use}-{source.value}-{category.value}-{language.value}'\n","    print(f'logs_path: {logs_path}')\n"," \n","    preprocessed_data_df = data_df.copy()\n","    preprocessed_data_df['target'] = preprocessed_data_df.apply(lambda row: define_target(row, category.value), axis=1)\n"," \n","    validation_df = preprocessed_data_df[((preprocessed_data_df['lang'] == language.value) & \n","                                          (preprocessed_data_df['source'] == source.value) & \n","                                          (preprocessed_data_df['is_test'] == 1))].reset_index()\n"," \n","    if multilingual_model:\n","      train_df = preprocessed_data_df[preprocessed_data_df['lang'] != language.value].reset_index()\n","    else:\n","      train_df = preprocessed_data_df[(preprocessed_data_df['lang'] == language.value) & \n","                                      (preprocessed_data_df['source'] == source.value) & \n","                                      (preprocessed_data_df['is_test'] == 0)].reset_index()\n"," \n","    print(f'training dataset size: {len(train_df)}')\n","    print(f'validation dataset size: {len(validation_df)}')\n","    \n","    print('******************************************************************')\n","\n","    #Configuration variables\n","    MAX_LEN = 200\n","    VALID_BATCH_SIZE = 32\n","    TRAIN_BATCH_SIZE = 32\n","    #LEARNING_RATE = trial.suggest_uniform(\"LEARNING_RATE\", 1e-05, 1e-04)\n","    #TRAIN_BATCH_SIZE = trial.suggest_categorical(\"TRAIN_BATCH_SIZE\", [16,32])\n","    #Model parameters\n","    gpus_to_use = [0]\n","\n","    #Hyperparameter optimization\n","    study_name = f'{experiment_name}/{model_to_use}-{source.value}-{category.value}-{language.value}'\n","    study = optuna.create_study(study_name = study_name, direction=\"maximize\")\n","    study.optimize(lambda trial: objective(trial, train_df, VALID_BATCH_SIZE, True, gpus_to_use, MAX_LEN, TRAIN_BATCH_SIZE), n_trials=10)\n","\n","    #Training best model for 2 epochs\n","    model_params = {\n","                    'learning_rate': study.best_params['LEARNING_RATE'], \n","                    'train_batch_size': TRAIN_BATCH_SIZE, \n","                    'validation_batch_size': VALID_BATCH_SIZE,\n","                    'shuffle': True\n","                    }\n","\n","    max_epochs = 2\n","    save_checkpoints = False\n","    early_stop = False\n","    print(f'------------ Training Best Model for {experiment_name}/{model_to_use}-{source.value}-{category.value}-{language.value}------------------------')\n","    best_f1 = train_model(train_df, validation_df, model_to_use, model_params,logs_path,undersampling,max_epochs,save_checkpoints,early_stop,gpus_to_use,MAX_LEN)\n","    print('******************************************************************')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PN3sJkfLDDFH"},"source":["## Read Data"]},{"cell_type":"code","metadata":{"id":"n2Tg2mGnNAUx"},"source":["data_location = root_PATH + '/data/preprocessed-AIRE-dataset/dataset-preprocessed.csv'\n","data_df = pd.read_csv(data_location).reset_index()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZgjWWkywIpGV"},"source":["# Experiments"]},{"cell_type":"markdown","metadata":{"id":"LCRcqxNgRtZM"},"source":["## BERT Monolingual"]},{"cell_type":"code","metadata":{"id":"Ka4leBkMdM4v"},"source":["multilingual_model = False\n","model_to_use_english = 'bert-base-uncased'\n","model_to_use_italian = 'dbmdz/bert-base-italian-cased'\n","\n","#Donwload of models and tokenizers if they are not in cache (so the donwload bar don't clutter the training logs)\n","BertTokenizer.from_pretrained(model_to_use_english)\n","BertModel.from_pretrained(model_to_use_english)\n","\n","BertTokenizer.from_pretrained(model_to_use_italian)\n","BertModel.from_pretrained(model_to_use_italian)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JegJMsf_ctrx"},"source":["### BERT Monolingual With Undersampling"]},{"cell_type":"code","metadata":{"id":"JPF9ytYSR0J4"},"source":["undersampling = True\n","experiment_name = 'BERT-monolingual-with-undersampling'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IiPq58xOhgQ9"},"source":["#English\n","language = Language.ENGLISH\n","model_to_use = model_to_use_english\n","\n","for source in Source:\n","  train_models_all_categories(language,source,model_to_use,data_df,undersampling,multilingual_model,experiment_name)\n","\n","gc.collect()\n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RreOlToMuSlq"},"source":["#Italian\n","language = Language.ITALIAN\n","source = Source.TWITTER\n","model_to_use = model_to_use_italian\n","\n","train_models_all_categories(language,source,model_to_use,data_df,undersampling, multilingual_model,experiment_name)\n","\n","gc.collect()\n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CLW2cjyfvGFS"},"source":["### BERT Monolingual **NO** Undersampling"]},{"cell_type":"code","metadata":{"id":"oUb8-qAuvI0r"},"source":["undersampling = False\n","experiment_name = 'BERT-monolingual-no-undersampling'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7_CROT5vvWJZ"},"source":["#English\n","language = Language.ENGLISH\n","model_to_use = model_to_use_english\n","\n","for source in Source:\n","  train_models_all_categories(language,source,model_to_use,data_df,undersampling,multilingual_model,experiment_name)\n","  \n","gc.collect()\n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MyMOwqI8vZVy"},"source":["#Italian\n","language = Language.ITALIAN\n","source = Source.TWITTER\n","model_to_use = model_to_use_italian\n","\n","train_models_all_categories(language,source,model_to_use,data_df,undersampling,multilingual_model,experiment_name)\n","\n","gc.collect()\n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bOF5lai7R9mD"},"source":["## BERT Multilingual"]},{"cell_type":"code","metadata":{"id":"xIVqaErVSEu1"},"source":["multilingual_model = True\n","model_to_use_english = 'bert-base-multilingual-uncased'\n","model_to_use_italian = 'bert-base-multilingual-uncased'\n","\n","#Donwload of models and tokenizers if they are not in cache (so the donwload bar don't clutter the training logs)\n","BertTokenizer.from_pretrained(model_to_use_english)\n","BertModel.from_pretrained(model_to_use_english)\n","\n","BertTokenizer.from_pretrained(model_to_use_italian)\n","BertModel.from_pretrained(model_to_use_italian)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eqfYU-ukDMfF"},"source":["### BERT Multilingual With Undersampling"]},{"cell_type":"code","metadata":{"id":"btbOu-x1H2D5"},"source":["undersampling = True\n","experiment_name = 'BERT-multilingual-with-undersampling'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eA2z6h54H2D6"},"source":["#English\n","language = Language.ENGLISH\n","model_to_use = model_to_use_english\n","\n","for source in Source:\n","  train_models_all_categories(language,source,model_to_use,data_df,undersampling,multilingual_model,experiment_name)\n","  \n","gc.collect()\n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0jiOkfVoH2D9"},"source":["#Italian\n","language = Language.ITALIAN\n","source = Source.TWITTER\n","model_to_use = model_to_use_italian\n","\n","train_models_all_categories(language,source,model_to_use,data_df,undersampling,multilingual_model,experiment_name)\n","\n","gc.collect()\n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dOv7SESdI5vx"},"source":["### BERT Multilingual **NO** Undersampling"]},{"cell_type":"code","metadata":{"id":"HVwtmCAJI5vy"},"source":["undersampling = False\n","experiment_name = 'BERT-multilingual-no-undersampling'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VIZnF0MsI5vz"},"source":["#English\n","language = Language.ENGLISH\n","model_to_use = model_to_use_english\n","\n","for source in Source:\n","  train_models_all_categories(language,source,model_to_use,data_df,undersampling,multilingual_model,experiment_name)\n","  \n","gc.collect()\n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8dkg_P4dI5v0"},"source":["#Italian\n","language = Language.ITALIAN\n","source = Source.TWITTER\n","model_to_use = model_to_use_italian\n","\n","train_models_all_categories(language,source,model_to_use,data_df,undersampling,multilingual_model,experiment_name)\n","\n","gc.collect()\n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]}]}